{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#필사 및 분석한 코드 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "from functools import partial, lru_cache\n",
    "\n",
    "import emoji\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class SequenceBucketCollator():\n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        \n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        \n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n",
    "    \n",
    "        return batch\n",
    "\n",
    "\n",
    "CUSTOM_TABLE = str.maketrans(\n",
    "    {\n",
    "        \"\\xad\": None,\n",
    "        \"\\x7f\": None,\n",
    "        \"\\ufeff\": None,\n",
    "        \"\\u200b\": None,\n",
    "        \"\\u200e\": None,\n",
    "        \"\\u202a\": None,\n",
    "        \"\\u202c\": None,\n",
    "        \"‘\": \"'\",\n",
    "        \"’\": \"'\",\n",
    "        \"`\": \"'\",\n",
    "        \"“\": '\"',\n",
    "        \"”\": '\"',\n",
    "        \"«\": '\"',\n",
    "        \"»\": '\"',\n",
    "        \"ɢ\": \"G\",\n",
    "        \"ɪ\": \"I\",\n",
    "        \"ɴ\": \"N\",\n",
    "        \"ʀ\": \"R\",\n",
    "        \"ʏ\": \"Y\",\n",
    "        \"ʙ\": \"B\",\n",
    "        \"ʜ\": \"H\",\n",
    "        \"ʟ\": \"L\",\n",
    "        \"ғ\": \"F\",\n",
    "        \"ᴀ\": \"A\",\n",
    "        \"ᴄ\": \"C\",\n",
    "        \"ᴅ\": \"D\",\n",
    "        \"ᴇ\": \"E\",\n",
    "        \"ᴊ\": \"J\",\n",
    "        \"ᴋ\": \"K\",\n",
    "        \"ᴍ\": \"M\",\n",
    "        \"Μ\": \"M\",\n",
    "        \"ᴏ\": \"O\",\n",
    "        \"ᴘ\": \"P\",\n",
    "        \"ᴛ\": \"T\",\n",
    "        \"ᴜ\": \"U\",\n",
    "        \"ᴡ\": \"W\",\n",
    "        \"ᴠ\": \"V\",\n",
    "        \"ĸ\": \"K\",\n",
    "        \"в\": \"B\",\n",
    "        \"м\": \"M\",\n",
    "        \"н\": \"H\",\n",
    "        \"т\": \"T\",\n",
    "        \"ѕ\": \"S\",\n",
    "        \"—\": \"-\",\n",
    "        \"–\": \"-\",\n",
    "    }\n",
    ")\n",
    "\n",
    "WORDS_REPLACER = [\n",
    "    (\"sh*t\", \"shit\"),\n",
    "    (\"s**t\", \"shit\"),\n",
    "    (\"f*ck\", \"fuck\"),\n",
    "    (\"fu*k\", \"fuck\"),\n",
    "    (\"f**k\", \"fuck\"),\n",
    "    (\"f*****g\", \"fucking\"),\n",
    "    (\"f***ing\", \"fucking\"),\n",
    "    (\"f**king\", \"fucking\"),\n",
    "    (\"p*ssy\", \"pussy\"),\n",
    "    (\"p***y\", \"pussy\"),\n",
    "    (\"pu**y\", \"pussy\"),\n",
    "    (\"p*ss\", \"piss\"),\n",
    "    (\"b*tch\", \"bitch\"),\n",
    "    (\"bit*h\", \"bitch\"),\n",
    "    (\"h*ll\", \"hell\"),\n",
    "    (\"h**l\", \"hell\"),\n",
    "    (\"cr*p\", \"crap\"),\n",
    "    (\"d*mn\", \"damn\"),\n",
    "    (\"stu*pid\", \"stupid\"),\n",
    "    (\"st*pid\", \"stupid\"),\n",
    "    (\"n*gger\", \"nigger\"),\n",
    "    (\"n***ga\", \"nigger\"),\n",
    "    (\"f*ggot\", \"faggot\"),\n",
    "    (\"scr*w\", \"screw\"),\n",
    "    (\"pr*ck\", \"prick\"),\n",
    "    (\"g*d\", \"god\"),\n",
    "    (\"s*x\", \"sex\"),\n",
    "    (\"a*s\", \"ass\"),\n",
    "    (\"a**hole\", \"asshole\"),\n",
    "    (\"a***ole\", \"asshole\"),\n",
    "    (\"a**\", \"ass\"),\n",
    "]\n",
    "\n",
    "REGEX_REPLACER = [\n",
    "    (re.compile(pat.replace(\"*\", \"\\*\"), flags=re.IGNORECASE), repl)\n",
    "    for pat, repl in WORDS_REPLACER\n",
    "]\n",
    "\n",
    "RE_SPACE = re.compile(r\"\\s\")\n",
    "RE_MULTI_SPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "NMS_TABLE = dict.fromkeys(\n",
    "    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == \"Mn\"\n",
    ")\n",
    "\n",
    "HEBREW_TABLE = {i: \"א\" for i in range(0x0590, 0x05FF)}\n",
    "ARABIC_TABLE = {i: \"ا\" for i in range(0x0600, 0x06FF)}\n",
    "CHINESE_TABLE = {i: \"是\" for i in range(0x4E00, 0x9FFF)}\n",
    "KANJI_TABLE = {i: \"ッ\" for i in range(0x2E80, 0x2FD5)}\n",
    "HIRAGANA_TABLE = {i: \"ッ\" for i in range(0x3041, 0x3096)}\n",
    "KATAKANA_TABLE = {i: \"ッ\" for i in range(0x30A0, 0x30FF)}\n",
    "\n",
    "TABLE = dict()\n",
    "TABLE.update(CUSTOM_TABLE)\n",
    "TABLE.update(NMS_TABLE)\n",
    "# Non-english languages\n",
    "TABLE.update(CHINESE_TABLE)\n",
    "TABLE.update(HEBREW_TABLE)\n",
    "TABLE.update(ARABIC_TABLE)\n",
    "TABLE.update(HIRAGANA_TABLE)\n",
    "TABLE.update(KATAKANA_TABLE)\n",
    "TABLE.update(KANJI_TABLE)\n",
    "\n",
    "\n",
    "EMOJI_REGEXP = emoji.get_emoji_regexp()\n",
    "\n",
    "UNICODE_EMOJI_MY = {\n",
    "    k: f\" EMJ {v.strip(':').replace('_', ' ')} \"\n",
    "    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()\n",
    "}\n",
    "\n",
    "\n",
    "def my_demojize(string: str) -> str:\n",
    "    def replace(match):\n",
    "        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))\n",
    "\n",
    "    return re.sub(\"\\ufe0f\", \"\", EMOJI_REGEXP.sub(replace, string))\n",
    "\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    text = my_demojize(text)\n",
    "\n",
    "    text = RE_SPACE.sub(\" \", text)\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    text = text.translate(TABLE)\n",
    "    text = RE_MULTI_SPACE.sub(\" \", text).strip()\n",
    "\n",
    "    for pattern, repl in REGEX_REPLACER:\n",
    "        text = pattern.sub(repl, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "PORTER_STEMMER = PorterStemmer()\n",
    "LANCASTER_STEMMER = LancasterStemmer()\n",
    "SNOWBALL_STEMMER = SnowballStemmer(\"english\")\n",
    "\n",
    "def word_forms(word):\n",
    "    yield word\n",
    "    yield word.lower()\n",
    "    yield word.upper()\n",
    "    yield word.capitalize()\n",
    "    yield PORTER_STEMMER.stem(word)\n",
    "    yield LANCASTER_STEMMER.stem(word)\n",
    "    yield SNOWBALL_STEMMER.stem(word)\n",
    "    \n",
    "def maybe_get_embedding(word, model):\n",
    "    for form in word_forms(word):\n",
    "        if form in model:\n",
    "            return model[form]\n",
    "\n",
    "    word = word.strip(\"-'\")\n",
    "    for form in word_forms(word):\n",
    "        if form in model:\n",
    "            return model[form]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def gensim_to_embedding_matrix(word2index, path):\n",
    "    model = KeyedVectors.load(path, mmap=\"r\")\n",
    "    embedding_matrix = np.zeros((max(word2index.values()) + 1, model.vector_size), dtype=np.float32)\n",
    "    unknown_words = []\n",
    "\n",
    "    for word, i in word2index.items():\n",
    "        maybe_embedding = maybe_get_embedding(word, model)\n",
    "        if maybe_embedding is not None:\n",
    "            embedding_matrix[i] = maybe_embedding\n",
    "        else:\n",
    "            unknown_words.append(word)\n",
    "\n",
    "    return embedding_matrix, unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 307 ms, sys: 68.2 ms, total: 375 ms\n",
      "Wall time: 891 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = test.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 150 ms, sys: 171 ms, total: 322 ms\n",
      "Wall time: 58.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "     text_list = pool.map(normalize, test.comment_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.2 s, sys: 144 ms, total: 19.4 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "test_word_sequences = []\n",
    "word_dict = {}\n",
    "word_index = 1\n",
    "\n",
    "for doc in text_list:\n",
    "    word_seq = []\n",
    "    for token in tknzr.tokenize(doc):\n",
    "        if token not in word_dict:\n",
    "            word_dict[token] = word_index\n",
    "            word_index += 1\n",
    "        word_seq.append(word_dict[token])\n",
    "    test_word_sequences.append(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len = 311\n",
      "CPU times: user 1.66 s, sys: 302 ms, total: 1.96 s\n",
      "Wall time: 1.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_lengths = torch.from_numpy(np.array([len(x) for x in test_word_sequences]))\n",
    "maxlen = test_lengths.max() \n",
    "print(f\"Max len = {maxlen}\")\n",
    "maxlen = min(maxlen, 400)\n",
    "\n",
    "x_test_padded = torch.tensor(pad_sequences(test_word_sequences, maxlen=maxlen)).long()\n",
    "test_collator = SequenceBucketCollator(torch.max, sequence_index=0, length_index=1)\n",
    "\n",
    "del text_list, test_word_sequences, tknzr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.9 s, sys: 7.56 s, total: 41.5 s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "glove_matrix, _ = gensim_to_embedding_matrix(\n",
    "    word_dict,\n",
    "    \"../input/gensim-embeddings-dataset/glove.840B.300d.gensim\",\n",
    ")\n",
    "\n",
    "crawl_matrix, _ = gensim_to_embedding_matrix(\n",
    "    word_dict, \n",
    "    \"../input/gensim-embeddings-dataset/crawl-300d-2M.gensim\",\n",
    ")\n",
    "\n",
    "para_matrix, _ = gensim_to_embedding_matrix(\n",
    "    word_dict, \n",
    "    \"../input/gensim-embeddings-dataset/paragram_300_sl999.gensim\",\n",
    ")\n",
    "\n",
    "w2v_matrix, _ = gensim_to_embedding_matrix(\n",
    "    word_dict, \n",
    "    \"../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.20.0 when using version 0.21.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 698 ms, sys: 79.5 ms, total: 777 ms\n",
      "Wall time: 784 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def one_hot_char_embeddings(word2index, char_vectorizer):\n",
    "    words = [\"\"] * (max(word2index.values()) + 1)\n",
    "    for word, i in word2index.items():\n",
    "        words[i] = word\n",
    "\n",
    "    return char_vectorizer.transform(words).toarray().astype(np.float32)\n",
    "\n",
    "char_matrix = one_hot_char_embeddings(\n",
    "    word_dict,\n",
    "    joblib.load('../input/jigsaw-solution-ver-1/char_vectorizer.pkl'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 6 * LSTM_UNITS\n",
    "\n",
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = super(SpatialDropout, self).forward(x)\n",
    "        x = x.permute(0, 3, 2, 1)\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_matrix, output_aux_sub=11):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        embed_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding(embedding_matrix.shape[0], embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.embedding_dropout = SpatialDropout(0.3)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n",
    "    \n",
    "        self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n",
    "        \n",
    "        self.linear_out = nn.Linear(DENSE_HIDDEN_UNITS + 6 + output_aux_sub, 1)\n",
    "        self.linear_aux_out = nn.Linear(DENSE_HIDDEN_UNITS, 6)\n",
    "        self.linear_sub_out = nn.Linear(DENSE_HIDDEN_UNITS, output_aux_sub)\n",
    "        \n",
    "    def forward(self, x, lengths=None):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = self.embedding_dropout(h_embedding)\n",
    "        \n",
    "        h_lstm1, _ = self.lstm1(h_embedding)\n",
    "        h_lstm2, _ = self.lstm2(h_lstm1)\n",
    "        \n",
    "        avg_pool1 = torch.mean(h_lstm1, 1)\n",
    "        avg_pool2 = torch.mean(h_lstm2, 1)\n",
    "        max_pool2, _ = torch.max(h_lstm2, 1)\n",
    "        \n",
    "        h_conc = torch.cat((avg_pool1, max_pool2, avg_pool2), 1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(h_conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(h_conc))\n",
    "        \n",
    "        hidden = h_conc + h_conc_linear1 + h_conc_linear2\n",
    "\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        sub_result = self.linear_sub_out(hidden)\n",
    "        result = self.linear_out(torch.cat((hidden, aux_result, sub_result), 1))\n",
    "        out = torch.cat([result, aux_result, sub_result], 1)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def get_lstm_preds(model_name, embedding_matrix):\n",
    "    model = NeuralNet(embedding_matrix)\n",
    "    temp_dict = torch.load('../input/jigsaw-solution-ver-1/' + model_name)\n",
    "    temp_dict['embedding.weight'] = torch.tensor(embedding_matrix)\n",
    "    model.load_state_dict(temp_dict)\n",
    "    model = model.cuda()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad=False\n",
    "    model = model.eval()\n",
    "    \n",
    "    batch_size = 256\n",
    "    test_dataset = data.TensorDataset(x_test_padded, test_lengths)\n",
    "    test_loader = data.DataLoader(test_dataset, batch_size=batch_size, collate_fn=test_collator)\n",
    "    \n",
    "    preds = np.zeros((len(test_dataset), 18), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for i, (x_batch) in enumerate(test_loader):\n",
    "            X_1 = x_batch[0].cuda()\n",
    "            y_pred = torch.sigmoid(model(X_1)).cpu().numpy()\n",
    "            preds[i * batch_size:(i + 1) * batch_size] = y_pred\n",
    "            \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook_100_5.bin\n",
      "Notebook_100_6.bin\n",
      "Notebook_100_7.bin\n",
      "Notebook_100_8.bin\n",
      "Notebook_100_9.bin\n",
      "Notebook_100_10.bin\n",
      "Notebook_100_11.bin\n",
      "Notebook_100_12.bin\n",
      "CPU times: user 2min 59s, sys: 8.66 s, total: 3min 8s\n",
      "Wall time: 2min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lstm_models = ['Notebook_100_5.bin',\n",
    "               'Notebook_100_6.bin', \n",
    "               'Notebook_100_7.bin',\n",
    "               'Notebook_100_8.bin',\n",
    "               'Notebook_100_9.bin', \n",
    "               'Notebook_100_10.bin', \n",
    "               'Notebook_100_11.bin',\n",
    "               'Notebook_100_12.bin']\n",
    "\n",
    "embedding_matrix = np.concatenate([glove_matrix, crawl_matrix, w2v_matrix, char_matrix], axis=1)\n",
    "\n",
    "all_lstm_preds = []\n",
    "for model_name in lstm_models:\n",
    "    print(model_name)\n",
    "    preds = get_lstm_preds(model_name, embedding_matrix)\n",
    "    all_lstm_preds.append(preds)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_models = ['Notebook_100_13.bin']\n",
    "\n",
    "# embedding_matrix = np.concatenate([glove_matrix, crawl_matrix, para_matrix, char_matrix], axis=1)\n",
    "\n",
    "# for model_name in lstm_models:\n",
    "#     print(model_name)\n",
    "#     preds = get_lstm_preds(model_name, embedding_matrix)\n",
    "#     all_lstm_preds.append(preds)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook_100_1.bin\n",
      "Notebook_100_2.bin\n",
      "Notebook_100_3.bin\n",
      "Notebook_100_4.bin\n",
      "CPU times: user 1min 29s, sys: 4.86 s, total: 1min 34s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lstm_models = ['Notebook_100_1.bin']\n",
    "\n",
    "embedding_matrix = np.concatenate([para_matrix, crawl_matrix, w2v_matrix, char_matrix], axis=1)\n",
    "\n",
    "for model_name in lstm_models:\n",
    "    print(model_name)\n",
    "    preds = get_lstm_preds(model_name, embedding_matrix)\n",
    "    all_lstm_preds.append(preds)\n",
    "    \n",
    "lstm_models = ['Notebook_100_2.bin']\n",
    "\n",
    "embedding_matrix = np.concatenate([glove_matrix, crawl_matrix, w2v_matrix, char_matrix], axis=1)\n",
    "\n",
    "for model_name in lstm_models:\n",
    "    print(model_name)\n",
    "    preds = get_lstm_preds(model_name, embedding_matrix)\n",
    "    all_lstm_preds.append(preds)\n",
    "    \n",
    "lstm_models = ['Notebook_100_3.bin']\n",
    "\n",
    "embedding_matrix = np.concatenate([glove_matrix, para_matrix, w2v_matrix, char_matrix], axis=1)\n",
    "\n",
    "for model_name in lstm_models:\n",
    "    print(model_name)\n",
    "    preds = get_lstm_preds(model_name, embedding_matrix)\n",
    "    all_lstm_preds.append(preds)\n",
    "\n",
    "lstm_models = ['Notebook_100_4.bin']\n",
    "\n",
    "embedding_matrix = np.concatenate([glove_matrix, para_matrix, crawl_matrix, char_matrix], axis=1)\n",
    "\n",
    "for model_name in lstm_models:\n",
    "    print(model_name)\n",
    "    preds = get_lstm_preds(model_name, embedding_matrix)\n",
    "    all_lstm_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_magic(preds):\n",
    "    return (\n",
    "        preds[:, 0] + preds[:, 1] * 0.05 - preds[:, -1] * 0.05 - preds[:, 4] * 0.05\n",
    "    )\n",
    "\n",
    "def sophisticated_magic(preds):\n",
    "    return (\n",
    "        preds[:, 0] + preds[:, 1] * 0.05 - preds[:, -1] * 0.03 - preds[:, 4] * 0.03\n",
    "    ) - preds[:, 14] * (1 - preds[:, 0]) * 0.05 - preds[:, 10] * (1 - preds[:, 0]) * 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "all_lstm_preds = np.vstack([sophisticated_magic(x) for x in all_lstm_preds])\n",
    "# preds_lstm = np.median(all_lstm_preds, axis=0)  # MEDIAN ensemble\n",
    "# preds_lstm = stats.trim_mean(all_lstm_preds, 0.1, axis=0) # 10%-trimmed-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE = {\n",
    "    'lstm': all_lstm_preds.mean(0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del crawl_matrix, glove_matrix, para_matrix, w2v_matrix, embedding_matrix\n",
    "del test_lengths, maxlen, x_test_padded, test_collator\n",
    "del lstm_models, all_lstm_preds\n",
    "del word_dict, word_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "\n",
    "UNCASED_BERT_MODEL_PATH = \"../input/transformer-tokenizers/bert-base-uncased/\"\n",
    "CASED_BERT_MODEL_PATH = \"../input/transformer-tokenizers/bert-base-cased/\"\n",
    "\n",
    "\n",
    "def clip_to_max_len(batch):\n",
    "    X, lengths = map(torch.stack, zip(*batch))\n",
    "    max_len = torch.max(lengths).item()\n",
    "    return X[:, :max_len]\n",
    "\n",
    "def prepare_bert(bin_file, is_cased):\n",
    "    bert_path = CASED_BERT_MODEL_PATH if is_cased else UNCASED_BERT_MODEL_PATH \n",
    "    config = BertConfig(os.path.join(bert_path, \"bert_config.json\"))\n",
    "    model = BertForSequenceClassification(config, num_labels=18)\n",
    "    model.load_state_dict(torch.load(bin_file, map_location=\"cpu\"))\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "    return model\n",
    "\n",
    "def prepare_tokenizer(is_cased):\n",
    "    if is_cased:\n",
    "        return BertTokenizer.from_pretrained(CASED_BERT_MODEL_PATH, do_lower_case=False)\n",
    "    else:\n",
    "        return BertTokenizer.from_pretrained(UNCASED_BERT_MODEL_PATH, do_lower_case=True)\n",
    "    \n",
    "    \n",
    "def apply_bert(model, loader):\n",
    "    preds = np.zeros((len(loader.dataset), 18), dtype=np.float32)\n",
    "    for i, X in enumerate(loader):\n",
    "        X = X.cuda()\n",
    "        p = torch.sigmoid(model(X, attention_mask=(X > 0)))\n",
    "        preds[i * loader.batch_size : (i + 1) * loader.batch_size] = p.cpu().numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNCASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.04 s, sys: 1.02 s, total: 6.06 s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = prepare_tokenizer(is_cased=False)\n",
    "MAX_LEN = 400 - 2\n",
    "\n",
    "def convert_line(text):\n",
    "    tokens_a = tokenizer.tokenize(text)[:MAX_LEN]\n",
    "    one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"])\n",
    "    one_token += [0] * (MAX_LEN - len(tokens_a))\n",
    "    return one_token\n",
    "\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    sequences = pool.map(convert_line, test.comment_text)    \n",
    "sequences = np.array(sequences)\n",
    "\n",
    "lengths = np.argmax(sequences == 0, axis=1)\n",
    "lengths[lengths == 0] = sequences.shape[1]\n",
    "\n",
    "ids = lengths.argsort(kind=\"stable\")\n",
    "inverse_ids = test.id.values[ids].argsort(kind=\"stable\")\n",
    "\n",
    "sequences = torch.from_numpy(sequences)\n",
    "lengths = torch.from_numpy(lengths)\n",
    "\n",
    "test_dataset = data.TensorDataset(sequences, lengths)\n",
    "test_loader = data.DataLoader(data.Subset(test_dataset, ids), batch_size=128, collate_fn=clip_to_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 13s, sys: 5.53 s, total: 43min 18s\n",
      "Wall time: 43min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "UNCASED_BERT_MODELS = [\n",
    "    \"BERT_exp_BERT_3_decay_5_epoch_1\",\n",
    "    \"final-pipe1-raw_1\",\n",
    "    \"final-pipe1-raw_2\",\n",
    "    \"final-pipe1-raw_3\",\n",
    "    \"final-pipe2-raw_1\",\n",
    "    \"final-pipe2-raw_2\",\n",
    "    \"final-pipe2-raw_3\",\n",
    "    \"final-pipe4-wiki_raw_1\",\n",
    "    \"final-pipe4-wiki_raw_2\",\n",
    "    \"final-pipe4-wiki_raw_3\",\n",
    "]\n",
    "\n",
    "for path in UNCASED_BERT_MODELS:\n",
    "    model = prepare_bert(f\"../input/toxic-models-zoo/{path}.bin\", is_cased=False)\n",
    "    preds = apply_bert(model, test_loader)\n",
    "    STORE[path] = sophisticated_magic(preds)[inverse_ids]\n",
    "    \n",
    "    \n",
    "for path in [\"final-pipe2-raw_4\", \"BERT_exp_BERT_3_epoch_1\"]:\n",
    "    model = prepare_bert(f\"../input/jigsaw-solution-ver-1/{path}.bin\", is_cased=False)\n",
    "    preds = apply_bert(model, test_loader)\n",
    "    STORE[path] = sophisticated_magic(preds)[inverse_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.3 s, sys: 1.03 s, total: 6.33 s\n",
      "Wall time: 45.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer = prepare_tokenizer(is_cased=True)\n",
    "\n",
    "def convert_line(text):\n",
    "    tokens_a = tokenizer.tokenize(text)[:MAX_LEN]\n",
    "    one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + tokens_a + [\"[SEP]\"])\n",
    "    one_token += [0] * (MAX_LEN - len(tokens_a))\n",
    "    return one_token\n",
    "\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    sequences = pool.map(convert_line, test.comment_text)    \n",
    "sequences = np.array(sequences)\n",
    "\n",
    "lengths = np.argmax(sequences == 0, axis=1)\n",
    "lengths[lengths == 0] = sequences.shape[1]\n",
    "\n",
    "ids = lengths.argsort(kind=\"stable\")\n",
    "inverse_ids = test.id.values[ids].argsort(kind=\"stable\")\n",
    "\n",
    "sequences = torch.from_numpy(sequences)\n",
    "lengths = torch.from_numpy(lengths)\n",
    "\n",
    "test_dataset = data.TensorDataset(sequences, lengths)\n",
    "test_loader = data.DataLoader(data.Subset(test_dataset, ids), batch_size=128, collate_fn=clip_to_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 44s, sys: 3.83 s, total: 29min 48s\n",
      "Wall time: 30min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "CASED_BERT_MODELS = [\n",
    "    \"BERT_exp_BERT_3_cased_decay_epoch_1\",\n",
    "    \"final-pipe2-cased_1\",\n",
    "    \"final-pipe2-cased_2\",\n",
    "    \"final-pipe2-cased_3\",\n",
    "#     \"final-pipe2-cased_4\",\n",
    "#     \"final-pipe2-cased_5\",\n",
    "#     \"final-pipe2-cased_6\",\n",
    "    \"final-pipe5-wiki_cased_1\",\n",
    "    \"final-pipe5-wiki_cased_2\",\n",
    "    \"final-pipe5-wiki_cased_3\",\n",
    "]\n",
    "\n",
    "for path in CASED_BERT_MODELS:\n",
    "    model = prepare_bert(f\"../input/toxic-models-zoo/{path}.bin\", is_cased=True)\n",
    "    preds = apply_bert(model, test_loader)\n",
    "    STORE[path] = sophisticated_magic(preds)[inverse_ids]\n",
    "    \n",
    "    \n",
    "for path in [\"final-pipe2-cased_10\"]:\n",
    "    model = prepare_bert(f\"../input/jigsaw-solution-ver-1/{path}.bin\", is_cased=True)\n",
    "    preds = apply_bert(model, test_loader)\n",
    "    STORE[path] = sophisticated_magic(preds)[inverse_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT2CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from io import open\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class MonkeyPatchedGPT2Tokenizer(object):\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n",
    "        vocab_file = os.path.join(pretrained_model_name_or_path, \"vocab.json\")\n",
    "        merges_file = os.path.join(pretrained_model_name_or_path, \"merges.txt\")\n",
    "\n",
    "        max_len = 1024\n",
    "        kwargs['max_len'] = min(kwargs.get('max_len', int(1e12)), max_len)\n",
    "        # Instantiate tokenizer.\n",
    "        special_tokens = kwargs.pop('special_tokens', [])\n",
    "        tokenizer = cls(vocab_file, merges_file, special_tokens=special_tokens, *inputs, **kwargs)\n",
    "        return tokenizer\n",
    "\n",
    "    def __init__(self, vocab_file, merges_file, errors='replace', special_tokens=None, max_len=None):\n",
    "        self.max_len = max_len if max_len is not None else int(1e12)\n",
    "        self.encoder = json.load(open(vocab_file))\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        bpe_data = open(merges_file, encoding='utf-8').read().split('\\n')[1:-1]\n",
    "        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized  of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "        self.special_tokens = {}\n",
    "        self.special_tokens_decoder = {}\n",
    "        self.set_special_tokens(special_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder) + len(self.special_tokens)\n",
    "\n",
    "    def set_special_tokens(self, special_tokens):\n",
    "        if not special_tokens:\n",
    "            self.special_tokens = {}\n",
    "            self.special_tokens_decoder = {}\n",
    "            return\n",
    "        self.special_tokens = dict((tok, len(self.encoder) + i) for i, tok in enumerate(special_tokens))\n",
    "        self.special_tokens_decoder = {v:k for k, v in self.special_tokens.items()}\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        ids = []\n",
    "        if isinstance(tokens, str):\n",
    "            if tokens in self.special_tokens:\n",
    "                return self.special_tokens[tokens]\n",
    "            else:\n",
    "                return self.encoder.get(tokens, 0)\n",
    "        for token in tokens:\n",
    "            if token in self.special_tokens:\n",
    "                ids.append(self.special_tokens[token])\n",
    "            else:\n",
    "                ids.append(self.encoder.get(token, 0))\n",
    "        if len(ids) > self.max_len:\n",
    "            print(\n",
    "                \"Token indices sequence length is longer than the specified maximum \"\n",
    "                \" sequence length for this OpenAI GPT model ({} > {}). Running this\"\n",
    "                \" sequence through the model will result in indexing errors\".format(len(ids), self.max_len)\n",
    "            )\n",
    "        return ids\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n",
    "        \"\"\"Converts a sequence of ids in BPE tokens using the vocab.\"\"\"\n",
    "        tokens = []\n",
    "        for i in ids:\n",
    "            if i in self.special_tokens_decoder:\n",
    "                if not skip_special_tokens:\n",
    "                    tokens.append(self.special_tokens_decoder[i])\n",
    "            else:\n",
    "                tokens.append(self.decoder[i])\n",
    "        return tokens\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.convert_tokens_to_ids(self.tokenize(text))\n",
    "\n",
    "    def decode(self, tokens, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n",
    "        text = ''.join(self.convert_ids_to_tokens(tokens, skip_special_tokens=skip_special_tokens))\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        if clean_up_tokenization_spaces:\n",
    "            text = text.replace('<unk>', '')\n",
    "            text = text.replace(' .', '.').replace(' ?', '?').replace(' !', '!').replace(' ,', ','\n",
    "                    ).replace(\" ' \", \"'\").replace(\" n't\", \"n't\").replace(\" 'm\", \"'m\").replace(\" do not\", \" don't\"\n",
    "                    ).replace(\" 's\", \"'s\").replace(\" 've\", \"'ve\").replace(\" 're\", \"'re\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import GPT2Config, GPT2Model\n",
    "from pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel\n",
    "\n",
    "MAX_LEN = 250\n",
    "GPT2_TOKENIZER = MonkeyPatchedGPT2Tokenizer.from_pretrained(\"../input/transformer-tokenizers/gpt2/\")\n",
    "\n",
    "def convert_line_gpt2(text):\n",
    "    tokens_a = GPT2_TOKENIZER.tokenize(text)[:MAX_LEN]\n",
    "    one_token = GPT2_TOKENIZER.convert_tokens_to_ids(tokens_a)\n",
    "    one_token += [0] * (MAX_LEN - len(tokens_a))\n",
    "    return one_token\n",
    "\n",
    "\n",
    "def apply_gpt(model, loader):\n",
    "    preds = np.zeros((len(loader.dataset), 18), dtype=np.float32)\n",
    "    for i, X in enumerate(loader):\n",
    "        p = torch.sigmoid(model(X.cuda()))\n",
    "        preds[i * loader.batch_size : (i + 1) * loader.batch_size] = p.cpu().numpy()\n",
    "    return preds\n",
    "\n",
    "\n",
    "class GPT2CNN(GPT2PreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPT2Model(config)  \n",
    "        self.cnn1 = nn.Conv1d(768, 256, kernel_size=3, padding=1)\n",
    "        self.cnn2 = nn.Conv1d(256, num_labels, kernel_size=3, padding=1)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n",
    "        x, _ = self.transformer(input_ids, position_ids, token_type_ids, past)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.cnn1(x))\n",
    "        x = self.cnn2(x)\n",
    "        output, _ = torch.max(x, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.32 s, sys: 699 ms, total: 4.02 s\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with multiprocessing.Pool(processes=2) as pool:\n",
    "    sequences = np.array(pool.map(convert_line_gpt2, test.comment_text))\n",
    "\n",
    "lengths = np.argmax(sequences == 0, axis=1)\n",
    "lengths[lengths == 0] = sequences.shape[1]\n",
    "\n",
    "sequences = torch.from_numpy(sequences)\n",
    "lengths = torch.from_numpy(lengths)\n",
    "\n",
    "test_dataset = data.TensorDataset(sequences, lengths)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=64, collate_fn=clip_to_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27min 23s, sys: 2.08 s, total: 27min 25s\n",
      "Wall time: 27min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "GPT2CNN_MODELS = [\n",
    "    \"BERT_exp_GPT2_CNN_epoch_1\",\n",
    "    \"BERT_exp_GPT2_CNN_seed_epoch_1\",\n",
    "#     \"final-pipe6-gpt_wiki_1\",\n",
    "]\n",
    "\n",
    "for path in GPT2CNN_MODELS:\n",
    "    model = GPT2CNN(GPT2Config(), num_labels=18)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"../input/toxic-models-zoo/{path}.bin\", map_location=\"cpu\")\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model = model.eval()\n",
    "    model = model.cuda()\n",
    "    preds = apply_gpt(model, test_loader)\n",
    "    STORE[path] = sophisticated_magic(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(STORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lstm</th>\n",
       "      <th>BERT_exp_BERT_3_decay_5_epoch_1</th>\n",
       "      <th>final-pipe1-raw_1</th>\n",
       "      <th>final-pipe1-raw_2</th>\n",
       "      <th>final-pipe1-raw_3</th>\n",
       "      <th>final-pipe2-raw_1</th>\n",
       "      <th>final-pipe2-raw_2</th>\n",
       "      <th>final-pipe2-raw_3</th>\n",
       "      <th>final-pipe4-wiki_raw_1</th>\n",
       "      <th>final-pipe4-wiki_raw_2</th>\n",
       "      <th>final-pipe4-wiki_raw_3</th>\n",
       "      <th>final-pipe2-raw_4</th>\n",
       "      <th>BERT_exp_BERT_3_epoch_1</th>\n",
       "      <th>BERT_exp_BERT_3_cased_decay_epoch_1</th>\n",
       "      <th>final-pipe2-cased_1</th>\n",
       "      <th>final-pipe2-cased_2</th>\n",
       "      <th>final-pipe2-cased_3</th>\n",
       "      <th>final-pipe5-wiki_cased_1</th>\n",
       "      <th>final-pipe5-wiki_cased_2</th>\n",
       "      <th>final-pipe5-wiki_cased_3</th>\n",
       "      <th>final-pipe2-cased_10</th>\n",
       "      <th>BERT_exp_GPT2_CNN_epoch_1</th>\n",
       "      <th>BERT_exp_GPT2_CNN_seed_epoch_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.012383</td>\n",
       "      <td>0.012476</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>0.010217</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.010096</td>\n",
       "      <td>0.008282</td>\n",
       "      <td>0.009567</td>\n",
       "      <td>0.007860</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.011942</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.006158</td>\n",
       "      <td>0.006550</td>\n",
       "      <td>0.008221</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>0.007743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081519</td>\n",
       "      <td>0.088579</td>\n",
       "      <td>0.086486</td>\n",
       "      <td>0.074330</td>\n",
       "      <td>0.092897</td>\n",
       "      <td>0.082421</td>\n",
       "      <td>0.091878</td>\n",
       "      <td>0.077198</td>\n",
       "      <td>0.101560</td>\n",
       "      <td>0.083248</td>\n",
       "      <td>0.105542</td>\n",
       "      <td>0.087636</td>\n",
       "      <td>0.096387</td>\n",
       "      <td>0.077366</td>\n",
       "      <td>0.070397</td>\n",
       "      <td>0.084659</td>\n",
       "      <td>0.072126</td>\n",
       "      <td>0.089732</td>\n",
       "      <td>0.066629</td>\n",
       "      <td>0.067531</td>\n",
       "      <td>0.065960</td>\n",
       "      <td>0.106863</td>\n",
       "      <td>0.125449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.349261</td>\n",
       "      <td>0.353265</td>\n",
       "      <td>0.320458</td>\n",
       "      <td>0.329532</td>\n",
       "      <td>0.321700</td>\n",
       "      <td>0.338747</td>\n",
       "      <td>0.355912</td>\n",
       "      <td>0.346069</td>\n",
       "      <td>0.264668</td>\n",
       "      <td>0.297421</td>\n",
       "      <td>0.334132</td>\n",
       "      <td>0.291985</td>\n",
       "      <td>0.322148</td>\n",
       "      <td>0.258779</td>\n",
       "      <td>0.233789</td>\n",
       "      <td>0.202783</td>\n",
       "      <td>0.245383</td>\n",
       "      <td>0.192864</td>\n",
       "      <td>0.327538</td>\n",
       "      <td>0.214886</td>\n",
       "      <td>0.217010</td>\n",
       "      <td>0.266835</td>\n",
       "      <td>0.276162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041394</td>\n",
       "      <td>0.052269</td>\n",
       "      <td>0.069734</td>\n",
       "      <td>0.053456</td>\n",
       "      <td>0.046692</td>\n",
       "      <td>0.055645</td>\n",
       "      <td>0.052034</td>\n",
       "      <td>0.042089</td>\n",
       "      <td>0.055890</td>\n",
       "      <td>0.055489</td>\n",
       "      <td>0.060314</td>\n",
       "      <td>0.042380</td>\n",
       "      <td>0.040038</td>\n",
       "      <td>0.051102</td>\n",
       "      <td>0.048257</td>\n",
       "      <td>0.056114</td>\n",
       "      <td>0.042320</td>\n",
       "      <td>0.047534</td>\n",
       "      <td>0.076478</td>\n",
       "      <td>0.062260</td>\n",
       "      <td>0.055633</td>\n",
       "      <td>0.057787</td>\n",
       "      <td>0.043571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013125</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>0.034699</td>\n",
       "      <td>0.037320</td>\n",
       "      <td>0.025701</td>\n",
       "      <td>0.024074</td>\n",
       "      <td>0.016147</td>\n",
       "      <td>0.031278</td>\n",
       "      <td>0.032585</td>\n",
       "      <td>0.024058</td>\n",
       "      <td>0.012134</td>\n",
       "      <td>0.014659</td>\n",
       "      <td>0.019971</td>\n",
       "      <td>0.018601</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.019239</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.011961</td>\n",
       "      <td>0.022589</td>\n",
       "      <td>0.025586</td>\n",
       "      <td>0.007670</td>\n",
       "      <td>0.010253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lstm               ...                BERT_exp_GPT2_CNN_seed_epoch_1\n",
       "0  0.012383               ...                                      0.007743\n",
       "1  0.081519               ...                                      0.125449\n",
       "2  0.349261               ...                                      0.276162\n",
       "3  0.041394               ...                                      0.043571\n",
       "4  0.013125               ...                                      0.010253\n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lstm</th>\n",
       "      <th>BERT_exp_BERT_3_decay_5_epoch_1</th>\n",
       "      <th>final-pipe1-raw_1</th>\n",
       "      <th>final-pipe1-raw_2</th>\n",
       "      <th>final-pipe1-raw_3</th>\n",
       "      <th>final-pipe2-raw_1</th>\n",
       "      <th>final-pipe2-raw_2</th>\n",
       "      <th>final-pipe2-raw_3</th>\n",
       "      <th>final-pipe4-wiki_raw_1</th>\n",
       "      <th>final-pipe4-wiki_raw_2</th>\n",
       "      <th>final-pipe4-wiki_raw_3</th>\n",
       "      <th>final-pipe2-raw_4</th>\n",
       "      <th>BERT_exp_BERT_3_epoch_1</th>\n",
       "      <th>BERT_exp_BERT_3_cased_decay_epoch_1</th>\n",
       "      <th>final-pipe2-cased_1</th>\n",
       "      <th>final-pipe2-cased_2</th>\n",
       "      <th>final-pipe2-cased_3</th>\n",
       "      <th>final-pipe5-wiki_cased_1</th>\n",
       "      <th>final-pipe5-wiki_cased_2</th>\n",
       "      <th>final-pipe5-wiki_cased_3</th>\n",
       "      <th>final-pipe2-cased_10</th>\n",
       "      <th>BERT_exp_GPT2_CNN_epoch_1</th>\n",
       "      <th>BERT_exp_GPT2_CNN_seed_epoch_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969450</td>\n",
       "      <td>0.969161</td>\n",
       "      <td>0.969274</td>\n",
       "      <td>0.970152</td>\n",
       "      <td>0.968926</td>\n",
       "      <td>0.968380</td>\n",
       "      <td>0.969601</td>\n",
       "      <td>0.969940</td>\n",
       "      <td>0.969398</td>\n",
       "      <td>0.969386</td>\n",
       "      <td>0.968493</td>\n",
       "      <td>0.970309</td>\n",
       "      <td>0.968216</td>\n",
       "      <td>0.970352</td>\n",
       "      <td>0.969861</td>\n",
       "      <td>0.969917</td>\n",
       "      <td>0.969650</td>\n",
       "      <td>0.970042</td>\n",
       "      <td>0.970168</td>\n",
       "      <td>0.969065</td>\n",
       "      <td>0.969316</td>\n",
       "      <td>0.968998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_BERT_3_decay_5_epoch_1</th>\n",
       "      <td>0.969450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992682</td>\n",
       "      <td>0.992108</td>\n",
       "      <td>0.992735</td>\n",
       "      <td>0.989387</td>\n",
       "      <td>0.988778</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.992526</td>\n",
       "      <td>0.991884</td>\n",
       "      <td>0.992203</td>\n",
       "      <td>0.988223</td>\n",
       "      <td>0.990316</td>\n",
       "      <td>0.979770</td>\n",
       "      <td>0.980338</td>\n",
       "      <td>0.980307</td>\n",
       "      <td>0.980777</td>\n",
       "      <td>0.980165</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.980323</td>\n",
       "      <td>0.979112</td>\n",
       "      <td>0.972000</td>\n",
       "      <td>0.971376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe1-raw_1</th>\n",
       "      <td>0.969161</td>\n",
       "      <td>0.992682</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992598</td>\n",
       "      <td>0.993055</td>\n",
       "      <td>0.990246</td>\n",
       "      <td>0.989669</td>\n",
       "      <td>0.990071</td>\n",
       "      <td>0.992834</td>\n",
       "      <td>0.992176</td>\n",
       "      <td>0.992764</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>0.990123</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>0.980942</td>\n",
       "      <td>0.980939</td>\n",
       "      <td>0.981242</td>\n",
       "      <td>0.980711</td>\n",
       "      <td>0.980844</td>\n",
       "      <td>0.980862</td>\n",
       "      <td>0.979554</td>\n",
       "      <td>0.971961</td>\n",
       "      <td>0.971356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe1-raw_2</th>\n",
       "      <td>0.969274</td>\n",
       "      <td>0.992108</td>\n",
       "      <td>0.992598</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>0.989777</td>\n",
       "      <td>0.989214</td>\n",
       "      <td>0.989752</td>\n",
       "      <td>0.992355</td>\n",
       "      <td>0.992263</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.988831</td>\n",
       "      <td>0.989554</td>\n",
       "      <td>0.979355</td>\n",
       "      <td>0.980863</td>\n",
       "      <td>0.980794</td>\n",
       "      <td>0.980872</td>\n",
       "      <td>0.980520</td>\n",
       "      <td>0.980667</td>\n",
       "      <td>0.980682</td>\n",
       "      <td>0.979486</td>\n",
       "      <td>0.971626</td>\n",
       "      <td>0.971007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe1-raw_3</th>\n",
       "      <td>0.970152</td>\n",
       "      <td>0.992735</td>\n",
       "      <td>0.993055</td>\n",
       "      <td>0.992739</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990079</td>\n",
       "      <td>0.989467</td>\n",
       "      <td>0.989986</td>\n",
       "      <td>0.992824</td>\n",
       "      <td>0.992432</td>\n",
       "      <td>0.993024</td>\n",
       "      <td>0.988771</td>\n",
       "      <td>0.990367</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.981242</td>\n",
       "      <td>0.981281</td>\n",
       "      <td>0.981615</td>\n",
       "      <td>0.980880</td>\n",
       "      <td>0.981124</td>\n",
       "      <td>0.981194</td>\n",
       "      <td>0.979778</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>0.971649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_1</th>\n",
       "      <td>0.968926</td>\n",
       "      <td>0.989387</td>\n",
       "      <td>0.990246</td>\n",
       "      <td>0.989777</td>\n",
       "      <td>0.990079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991962</td>\n",
       "      <td>0.992348</td>\n",
       "      <td>0.989888</td>\n",
       "      <td>0.989734</td>\n",
       "      <td>0.989932</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.991937</td>\n",
       "      <td>0.980074</td>\n",
       "      <td>0.981564</td>\n",
       "      <td>0.981680</td>\n",
       "      <td>0.982056</td>\n",
       "      <td>0.981424</td>\n",
       "      <td>0.981555</td>\n",
       "      <td>0.981521</td>\n",
       "      <td>0.980367</td>\n",
       "      <td>0.973138</td>\n",
       "      <td>0.972744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_2</th>\n",
       "      <td>0.968380</td>\n",
       "      <td>0.988778</td>\n",
       "      <td>0.989669</td>\n",
       "      <td>0.989214</td>\n",
       "      <td>0.989467</td>\n",
       "      <td>0.991962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991956</td>\n",
       "      <td>0.989185</td>\n",
       "      <td>0.989017</td>\n",
       "      <td>0.989401</td>\n",
       "      <td>0.990907</td>\n",
       "      <td>0.991206</td>\n",
       "      <td>0.979951</td>\n",
       "      <td>0.981352</td>\n",
       "      <td>0.981650</td>\n",
       "      <td>0.981834</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.981484</td>\n",
       "      <td>0.981417</td>\n",
       "      <td>0.980401</td>\n",
       "      <td>0.973306</td>\n",
       "      <td>0.972834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_3</th>\n",
       "      <td>0.969601</td>\n",
       "      <td>0.989527</td>\n",
       "      <td>0.990071</td>\n",
       "      <td>0.989752</td>\n",
       "      <td>0.989986</td>\n",
       "      <td>0.992348</td>\n",
       "      <td>0.991956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990031</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.989864</td>\n",
       "      <td>0.991165</td>\n",
       "      <td>0.992054</td>\n",
       "      <td>0.979998</td>\n",
       "      <td>0.981372</td>\n",
       "      <td>0.981751</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.981258</td>\n",
       "      <td>0.981398</td>\n",
       "      <td>0.981351</td>\n",
       "      <td>0.980286</td>\n",
       "      <td>0.972910</td>\n",
       "      <td>0.972583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe4-wiki_raw_1</th>\n",
       "      <td>0.969940</td>\n",
       "      <td>0.992526</td>\n",
       "      <td>0.992834</td>\n",
       "      <td>0.992355</td>\n",
       "      <td>0.992824</td>\n",
       "      <td>0.989888</td>\n",
       "      <td>0.989185</td>\n",
       "      <td>0.990031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992605</td>\n",
       "      <td>0.992809</td>\n",
       "      <td>0.988827</td>\n",
       "      <td>0.990204</td>\n",
       "      <td>0.979535</td>\n",
       "      <td>0.980884</td>\n",
       "      <td>0.981019</td>\n",
       "      <td>0.981136</td>\n",
       "      <td>0.980869</td>\n",
       "      <td>0.980873</td>\n",
       "      <td>0.980959</td>\n",
       "      <td>0.979546</td>\n",
       "      <td>0.971726</td>\n",
       "      <td>0.971218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe4-wiki_raw_2</th>\n",
       "      <td>0.969398</td>\n",
       "      <td>0.991884</td>\n",
       "      <td>0.992176</td>\n",
       "      <td>0.992263</td>\n",
       "      <td>0.992432</td>\n",
       "      <td>0.989734</td>\n",
       "      <td>0.989017</td>\n",
       "      <td>0.989583</td>\n",
       "      <td>0.992605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992488</td>\n",
       "      <td>0.988546</td>\n",
       "      <td>0.989591</td>\n",
       "      <td>0.979312</td>\n",
       "      <td>0.980743</td>\n",
       "      <td>0.980866</td>\n",
       "      <td>0.981156</td>\n",
       "      <td>0.980747</td>\n",
       "      <td>0.980809</td>\n",
       "      <td>0.980812</td>\n",
       "      <td>0.979355</td>\n",
       "      <td>0.971606</td>\n",
       "      <td>0.971064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe4-wiki_raw_3</th>\n",
       "      <td>0.969386</td>\n",
       "      <td>0.992203</td>\n",
       "      <td>0.992764</td>\n",
       "      <td>0.992273</td>\n",
       "      <td>0.993024</td>\n",
       "      <td>0.989932</td>\n",
       "      <td>0.989401</td>\n",
       "      <td>0.989864</td>\n",
       "      <td>0.992809</td>\n",
       "      <td>0.992488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.988705</td>\n",
       "      <td>0.989958</td>\n",
       "      <td>0.979413</td>\n",
       "      <td>0.980787</td>\n",
       "      <td>0.980949</td>\n",
       "      <td>0.981123</td>\n",
       "      <td>0.980843</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>0.980983</td>\n",
       "      <td>0.979486</td>\n",
       "      <td>0.971940</td>\n",
       "      <td>0.971272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_4</th>\n",
       "      <td>0.968493</td>\n",
       "      <td>0.988223</td>\n",
       "      <td>0.988826</td>\n",
       "      <td>0.988831</td>\n",
       "      <td>0.988771</td>\n",
       "      <td>0.991191</td>\n",
       "      <td>0.990907</td>\n",
       "      <td>0.991165</td>\n",
       "      <td>0.988827</td>\n",
       "      <td>0.988546</td>\n",
       "      <td>0.988705</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>0.979224</td>\n",
       "      <td>0.981060</td>\n",
       "      <td>0.981070</td>\n",
       "      <td>0.981295</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>0.980696</td>\n",
       "      <td>0.981086</td>\n",
       "      <td>0.979762</td>\n",
       "      <td>0.972691</td>\n",
       "      <td>0.972294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_BERT_3_epoch_1</th>\n",
       "      <td>0.970309</td>\n",
       "      <td>0.990316</td>\n",
       "      <td>0.990123</td>\n",
       "      <td>0.989554</td>\n",
       "      <td>0.990367</td>\n",
       "      <td>0.991937</td>\n",
       "      <td>0.991206</td>\n",
       "      <td>0.992054</td>\n",
       "      <td>0.990204</td>\n",
       "      <td>0.989591</td>\n",
       "      <td>0.989958</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980349</td>\n",
       "      <td>0.981261</td>\n",
       "      <td>0.981345</td>\n",
       "      <td>0.981597</td>\n",
       "      <td>0.980925</td>\n",
       "      <td>0.981013</td>\n",
       "      <td>0.981104</td>\n",
       "      <td>0.979696</td>\n",
       "      <td>0.973386</td>\n",
       "      <td>0.973041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_BERT_3_cased_decay_epoch_1</th>\n",
       "      <td>0.968216</td>\n",
       "      <td>0.979770</td>\n",
       "      <td>0.979553</td>\n",
       "      <td>0.979355</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.980074</td>\n",
       "      <td>0.979951</td>\n",
       "      <td>0.979998</td>\n",
       "      <td>0.979535</td>\n",
       "      <td>0.979312</td>\n",
       "      <td>0.979413</td>\n",
       "      <td>0.979224</td>\n",
       "      <td>0.980349</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989616</td>\n",
       "      <td>0.989764</td>\n",
       "      <td>0.989412</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>0.989064</td>\n",
       "      <td>0.989217</td>\n",
       "      <td>0.988064</td>\n",
       "      <td>0.974510</td>\n",
       "      <td>0.974068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_1</th>\n",
       "      <td>0.970352</td>\n",
       "      <td>0.980338</td>\n",
       "      <td>0.980942</td>\n",
       "      <td>0.980863</td>\n",
       "      <td>0.981242</td>\n",
       "      <td>0.981564</td>\n",
       "      <td>0.981352</td>\n",
       "      <td>0.981372</td>\n",
       "      <td>0.980884</td>\n",
       "      <td>0.980743</td>\n",
       "      <td>0.980787</td>\n",
       "      <td>0.981060</td>\n",
       "      <td>0.981261</td>\n",
       "      <td>0.989616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991217</td>\n",
       "      <td>0.991219</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.990634</td>\n",
       "      <td>0.990668</td>\n",
       "      <td>0.989705</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.974962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_2</th>\n",
       "      <td>0.969861</td>\n",
       "      <td>0.980307</td>\n",
       "      <td>0.980939</td>\n",
       "      <td>0.980794</td>\n",
       "      <td>0.981281</td>\n",
       "      <td>0.981680</td>\n",
       "      <td>0.981650</td>\n",
       "      <td>0.981751</td>\n",
       "      <td>0.981019</td>\n",
       "      <td>0.980866</td>\n",
       "      <td>0.980949</td>\n",
       "      <td>0.981070</td>\n",
       "      <td>0.981345</td>\n",
       "      <td>0.989764</td>\n",
       "      <td>0.991217</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991142</td>\n",
       "      <td>0.990838</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.990672</td>\n",
       "      <td>0.989528</td>\n",
       "      <td>0.975213</td>\n",
       "      <td>0.974694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_3</th>\n",
       "      <td>0.969917</td>\n",
       "      <td>0.980777</td>\n",
       "      <td>0.981242</td>\n",
       "      <td>0.980872</td>\n",
       "      <td>0.981615</td>\n",
       "      <td>0.982056</td>\n",
       "      <td>0.981834</td>\n",
       "      <td>0.981880</td>\n",
       "      <td>0.981136</td>\n",
       "      <td>0.981156</td>\n",
       "      <td>0.981123</td>\n",
       "      <td>0.981295</td>\n",
       "      <td>0.981597</td>\n",
       "      <td>0.989412</td>\n",
       "      <td>0.991219</td>\n",
       "      <td>0.991142</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990788</td>\n",
       "      <td>0.990749</td>\n",
       "      <td>0.990714</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.975480</td>\n",
       "      <td>0.975049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe5-wiki_cased_1</th>\n",
       "      <td>0.969650</td>\n",
       "      <td>0.980165</td>\n",
       "      <td>0.980711</td>\n",
       "      <td>0.980520</td>\n",
       "      <td>0.980880</td>\n",
       "      <td>0.981424</td>\n",
       "      <td>0.981231</td>\n",
       "      <td>0.981258</td>\n",
       "      <td>0.980869</td>\n",
       "      <td>0.980747</td>\n",
       "      <td>0.980843</td>\n",
       "      <td>0.980849</td>\n",
       "      <td>0.980925</td>\n",
       "      <td>0.989436</td>\n",
       "      <td>0.990826</td>\n",
       "      <td>0.990838</td>\n",
       "      <td>0.990788</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990571</td>\n",
       "      <td>0.990696</td>\n",
       "      <td>0.989435</td>\n",
       "      <td>0.974992</td>\n",
       "      <td>0.974493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe5-wiki_cased_2</th>\n",
       "      <td>0.970042</td>\n",
       "      <td>0.980100</td>\n",
       "      <td>0.980844</td>\n",
       "      <td>0.980667</td>\n",
       "      <td>0.981124</td>\n",
       "      <td>0.981555</td>\n",
       "      <td>0.981484</td>\n",
       "      <td>0.981398</td>\n",
       "      <td>0.980873</td>\n",
       "      <td>0.980809</td>\n",
       "      <td>0.980999</td>\n",
       "      <td>0.980696</td>\n",
       "      <td>0.981013</td>\n",
       "      <td>0.989064</td>\n",
       "      <td>0.990634</td>\n",
       "      <td>0.990762</td>\n",
       "      <td>0.990749</td>\n",
       "      <td>0.990571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990744</td>\n",
       "      <td>0.989371</td>\n",
       "      <td>0.975038</td>\n",
       "      <td>0.974562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe5-wiki_cased_3</th>\n",
       "      <td>0.970168</td>\n",
       "      <td>0.980323</td>\n",
       "      <td>0.980862</td>\n",
       "      <td>0.980682</td>\n",
       "      <td>0.981194</td>\n",
       "      <td>0.981521</td>\n",
       "      <td>0.981417</td>\n",
       "      <td>0.981351</td>\n",
       "      <td>0.980959</td>\n",
       "      <td>0.980812</td>\n",
       "      <td>0.980983</td>\n",
       "      <td>0.981086</td>\n",
       "      <td>0.981104</td>\n",
       "      <td>0.989217</td>\n",
       "      <td>0.990668</td>\n",
       "      <td>0.990672</td>\n",
       "      <td>0.990714</td>\n",
       "      <td>0.990696</td>\n",
       "      <td>0.990744</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989394</td>\n",
       "      <td>0.975542</td>\n",
       "      <td>0.975155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_10</th>\n",
       "      <td>0.969065</td>\n",
       "      <td>0.979112</td>\n",
       "      <td>0.979554</td>\n",
       "      <td>0.979486</td>\n",
       "      <td>0.979778</td>\n",
       "      <td>0.980367</td>\n",
       "      <td>0.980401</td>\n",
       "      <td>0.980286</td>\n",
       "      <td>0.979546</td>\n",
       "      <td>0.979355</td>\n",
       "      <td>0.979486</td>\n",
       "      <td>0.979762</td>\n",
       "      <td>0.979696</td>\n",
       "      <td>0.988064</td>\n",
       "      <td>0.989705</td>\n",
       "      <td>0.989528</td>\n",
       "      <td>0.989691</td>\n",
       "      <td>0.989435</td>\n",
       "      <td>0.989371</td>\n",
       "      <td>0.989394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.974444</td>\n",
       "      <td>0.974201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_GPT2_CNN_epoch_1</th>\n",
       "      <td>0.969316</td>\n",
       "      <td>0.972000</td>\n",
       "      <td>0.971961</td>\n",
       "      <td>0.971626</td>\n",
       "      <td>0.972392</td>\n",
       "      <td>0.973138</td>\n",
       "      <td>0.973306</td>\n",
       "      <td>0.972910</td>\n",
       "      <td>0.971726</td>\n",
       "      <td>0.971606</td>\n",
       "      <td>0.971940</td>\n",
       "      <td>0.972691</td>\n",
       "      <td>0.973386</td>\n",
       "      <td>0.974510</td>\n",
       "      <td>0.975361</td>\n",
       "      <td>0.975213</td>\n",
       "      <td>0.975480</td>\n",
       "      <td>0.974992</td>\n",
       "      <td>0.975038</td>\n",
       "      <td>0.975542</td>\n",
       "      <td>0.974444</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_GPT2_CNN_seed_epoch_1</th>\n",
       "      <td>0.968998</td>\n",
       "      <td>0.971376</td>\n",
       "      <td>0.971356</td>\n",
       "      <td>0.971007</td>\n",
       "      <td>0.971649</td>\n",
       "      <td>0.972744</td>\n",
       "      <td>0.972834</td>\n",
       "      <td>0.972583</td>\n",
       "      <td>0.971218</td>\n",
       "      <td>0.971064</td>\n",
       "      <td>0.971272</td>\n",
       "      <td>0.972294</td>\n",
       "      <td>0.973041</td>\n",
       "      <td>0.974068</td>\n",
       "      <td>0.974962</td>\n",
       "      <td>0.974694</td>\n",
       "      <td>0.975049</td>\n",
       "      <td>0.974493</td>\n",
       "      <td>0.974562</td>\n",
       "      <td>0.975155</td>\n",
       "      <td>0.974201</td>\n",
       "      <td>0.992179</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         lstm               ...                BERT_exp_GPT2_CNN_seed_epoch_1\n",
       "lstm                                 1.000000               ...                                      0.968998\n",
       "BERT_exp_BERT_3_decay_5_epoch_1      0.969450               ...                                      0.971376\n",
       "final-pipe1-raw_1                    0.969161               ...                                      0.971356\n",
       "final-pipe1-raw_2                    0.969274               ...                                      0.971007\n",
       "final-pipe1-raw_3                    0.970152               ...                                      0.971649\n",
       "final-pipe2-raw_1                    0.968926               ...                                      0.972744\n",
       "final-pipe2-raw_2                    0.968380               ...                                      0.972834\n",
       "final-pipe2-raw_3                    0.969601               ...                                      0.972583\n",
       "final-pipe4-wiki_raw_1               0.969940               ...                                      0.971218\n",
       "final-pipe4-wiki_raw_2               0.969398               ...                                      0.971064\n",
       "final-pipe4-wiki_raw_3               0.969386               ...                                      0.971272\n",
       "final-pipe2-raw_4                    0.968493               ...                                      0.972294\n",
       "BERT_exp_BERT_3_epoch_1              0.970309               ...                                      0.973041\n",
       "BERT_exp_BERT_3_cased_decay_epoch_1  0.968216               ...                                      0.974068\n",
       "final-pipe2-cased_1                  0.970352               ...                                      0.974962\n",
       "final-pipe2-cased_2                  0.969861               ...                                      0.974694\n",
       "final-pipe2-cased_3                  0.969917               ...                                      0.975049\n",
       "final-pipe5-wiki_cased_1             0.969650               ...                                      0.974493\n",
       "final-pipe5-wiki_cased_2             0.970042               ...                                      0.974562\n",
       "final-pipe5-wiki_cased_3             0.970168               ...                                      0.975155\n",
       "final-pipe2-cased_10                 0.969065               ...                                      0.974201\n",
       "BERT_exp_GPT2_CNN_epoch_1            0.969316               ...                                      0.992179\n",
       "BERT_exp_GPT2_CNN_seed_epoch_1       0.968998               ...                                      1.000000\n",
       "\n",
       "[23 rows x 23 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lstm</th>\n",
       "      <th>BERT_exp_BERT_3_decay_5_epoch_1</th>\n",
       "      <th>final-pipe1-raw_1</th>\n",
       "      <th>final-pipe1-raw_2</th>\n",
       "      <th>final-pipe1-raw_3</th>\n",
       "      <th>final-pipe2-raw_1</th>\n",
       "      <th>final-pipe2-raw_2</th>\n",
       "      <th>final-pipe2-raw_3</th>\n",
       "      <th>final-pipe4-wiki_raw_1</th>\n",
       "      <th>final-pipe4-wiki_raw_2</th>\n",
       "      <th>final-pipe4-wiki_raw_3</th>\n",
       "      <th>final-pipe2-raw_4</th>\n",
       "      <th>BERT_exp_BERT_3_epoch_1</th>\n",
       "      <th>BERT_exp_BERT_3_cased_decay_epoch_1</th>\n",
       "      <th>final-pipe2-cased_1</th>\n",
       "      <th>final-pipe2-cased_2</th>\n",
       "      <th>final-pipe2-cased_3</th>\n",
       "      <th>final-pipe5-wiki_cased_1</th>\n",
       "      <th>final-pipe5-wiki_cased_2</th>\n",
       "      <th>final-pipe5-wiki_cased_3</th>\n",
       "      <th>final-pipe2-cased_10</th>\n",
       "      <th>BERT_exp_GPT2_CNN_epoch_1</th>\n",
       "      <th>BERT_exp_GPT2_CNN_seed_epoch_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lstm</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.806990</td>\n",
       "      <td>0.807711</td>\n",
       "      <td>0.808439</td>\n",
       "      <td>0.809190</td>\n",
       "      <td>0.806045</td>\n",
       "      <td>0.806474</td>\n",
       "      <td>0.806853</td>\n",
       "      <td>0.806380</td>\n",
       "      <td>0.805461</td>\n",
       "      <td>0.804249</td>\n",
       "      <td>0.803756</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.804277</td>\n",
       "      <td>0.809930</td>\n",
       "      <td>0.809733</td>\n",
       "      <td>0.808587</td>\n",
       "      <td>0.805084</td>\n",
       "      <td>0.806114</td>\n",
       "      <td>0.805870</td>\n",
       "      <td>0.807446</td>\n",
       "      <td>0.811452</td>\n",
       "      <td>0.812112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_BERT_3_decay_5_epoch_1</th>\n",
       "      <td>0.806990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908482</td>\n",
       "      <td>0.904823</td>\n",
       "      <td>0.908004</td>\n",
       "      <td>0.888403</td>\n",
       "      <td>0.885999</td>\n",
       "      <td>0.889489</td>\n",
       "      <td>0.904472</td>\n",
       "      <td>0.899412</td>\n",
       "      <td>0.901883</td>\n",
       "      <td>0.882280</td>\n",
       "      <td>0.891546</td>\n",
       "      <td>0.849622</td>\n",
       "      <td>0.852054</td>\n",
       "      <td>0.852460</td>\n",
       "      <td>0.851922</td>\n",
       "      <td>0.850317</td>\n",
       "      <td>0.849869</td>\n",
       "      <td>0.849669</td>\n",
       "      <td>0.847389</td>\n",
       "      <td>0.826767</td>\n",
       "      <td>0.827394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe1-raw_1</th>\n",
       "      <td>0.807711</td>\n",
       "      <td>0.908482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908140</td>\n",
       "      <td>0.911034</td>\n",
       "      <td>0.891609</td>\n",
       "      <td>0.889217</td>\n",
       "      <td>0.891807</td>\n",
       "      <td>0.906177</td>\n",
       "      <td>0.902215</td>\n",
       "      <td>0.904847</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.889776</td>\n",
       "      <td>0.848055</td>\n",
       "      <td>0.853712</td>\n",
       "      <td>0.854265</td>\n",
       "      <td>0.853838</td>\n",
       "      <td>0.852047</td>\n",
       "      <td>0.851872</td>\n",
       "      <td>0.851820</td>\n",
       "      <td>0.849091</td>\n",
       "      <td>0.826036</td>\n",
       "      <td>0.826654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe1-raw_2</th>\n",
       "      <td>0.808439</td>\n",
       "      <td>0.904823</td>\n",
       "      <td>0.908140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908773</td>\n",
       "      <td>0.890085</td>\n",
       "      <td>0.887989</td>\n",
       "      <td>0.890543</td>\n",
       "      <td>0.903631</td>\n",
       "      <td>0.901207</td>\n",
       "      <td>0.901687</td>\n",
       "      <td>0.885158</td>\n",
       "      <td>0.886554</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.853072</td>\n",
       "      <td>0.854345</td>\n",
       "      <td>0.853852</td>\n",
       "      <td>0.851605</td>\n",
       "      <td>0.851446</td>\n",
       "      <td>0.850658</td>\n",
       "      <td>0.849410</td>\n",
       "      <td>0.825881</td>\n",
       "      <td>0.826499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe1-raw_3</th>\n",
       "      <td>0.809190</td>\n",
       "      <td>0.908004</td>\n",
       "      <td>0.911034</td>\n",
       "      <td>0.908773</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892997</td>\n",
       "      <td>0.890282</td>\n",
       "      <td>0.892992</td>\n",
       "      <td>0.906595</td>\n",
       "      <td>0.904317</td>\n",
       "      <td>0.906661</td>\n",
       "      <td>0.886347</td>\n",
       "      <td>0.890928</td>\n",
       "      <td>0.849205</td>\n",
       "      <td>0.855006</td>\n",
       "      <td>0.856488</td>\n",
       "      <td>0.855840</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.853480</td>\n",
       "      <td>0.852893</td>\n",
       "      <td>0.851066</td>\n",
       "      <td>0.826834</td>\n",
       "      <td>0.827519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_1</th>\n",
       "      <td>0.806045</td>\n",
       "      <td>0.888403</td>\n",
       "      <td>0.891609</td>\n",
       "      <td>0.890085</td>\n",
       "      <td>0.892997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900177</td>\n",
       "      <td>0.904032</td>\n",
       "      <td>0.888330</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.887361</td>\n",
       "      <td>0.894502</td>\n",
       "      <td>0.900654</td>\n",
       "      <td>0.849502</td>\n",
       "      <td>0.855018</td>\n",
       "      <td>0.855904</td>\n",
       "      <td>0.856051</td>\n",
       "      <td>0.853149</td>\n",
       "      <td>0.852943</td>\n",
       "      <td>0.852299</td>\n",
       "      <td>0.850682</td>\n",
       "      <td>0.825413</td>\n",
       "      <td>0.826050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_2</th>\n",
       "      <td>0.806474</td>\n",
       "      <td>0.885999</td>\n",
       "      <td>0.889217</td>\n",
       "      <td>0.887989</td>\n",
       "      <td>0.890282</td>\n",
       "      <td>0.900177</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901281</td>\n",
       "      <td>0.885890</td>\n",
       "      <td>0.884451</td>\n",
       "      <td>0.884942</td>\n",
       "      <td>0.892862</td>\n",
       "      <td>0.894597</td>\n",
       "      <td>0.847476</td>\n",
       "      <td>0.853384</td>\n",
       "      <td>0.854269</td>\n",
       "      <td>0.853989</td>\n",
       "      <td>0.851121</td>\n",
       "      <td>0.851525</td>\n",
       "      <td>0.850733</td>\n",
       "      <td>0.850135</td>\n",
       "      <td>0.823445</td>\n",
       "      <td>0.824052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_3</th>\n",
       "      <td>0.806853</td>\n",
       "      <td>0.889489</td>\n",
       "      <td>0.891807</td>\n",
       "      <td>0.890543</td>\n",
       "      <td>0.892992</td>\n",
       "      <td>0.904032</td>\n",
       "      <td>0.901281</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888387</td>\n",
       "      <td>0.886277</td>\n",
       "      <td>0.886662</td>\n",
       "      <td>0.895967</td>\n",
       "      <td>0.899611</td>\n",
       "      <td>0.848929</td>\n",
       "      <td>0.854534</td>\n",
       "      <td>0.855426</td>\n",
       "      <td>0.855193</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.851789</td>\n",
       "      <td>0.851812</td>\n",
       "      <td>0.850066</td>\n",
       "      <td>0.824968</td>\n",
       "      <td>0.825644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe4-wiki_raw_1</th>\n",
       "      <td>0.806380</td>\n",
       "      <td>0.904472</td>\n",
       "      <td>0.906177</td>\n",
       "      <td>0.903631</td>\n",
       "      <td>0.906595</td>\n",
       "      <td>0.888330</td>\n",
       "      <td>0.885890</td>\n",
       "      <td>0.888387</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906275</td>\n",
       "      <td>0.906939</td>\n",
       "      <td>0.882639</td>\n",
       "      <td>0.886186</td>\n",
       "      <td>0.845827</td>\n",
       "      <td>0.852044</td>\n",
       "      <td>0.852510</td>\n",
       "      <td>0.851869</td>\n",
       "      <td>0.853302</td>\n",
       "      <td>0.853201</td>\n",
       "      <td>0.853190</td>\n",
       "      <td>0.847535</td>\n",
       "      <td>0.823747</td>\n",
       "      <td>0.824667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe4-wiki_raw_2</th>\n",
       "      <td>0.805461</td>\n",
       "      <td>0.899412</td>\n",
       "      <td>0.902215</td>\n",
       "      <td>0.901207</td>\n",
       "      <td>0.904317</td>\n",
       "      <td>0.885870</td>\n",
       "      <td>0.884451</td>\n",
       "      <td>0.886277</td>\n",
       "      <td>0.906275</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.905052</td>\n",
       "      <td>0.881541</td>\n",
       "      <td>0.883277</td>\n",
       "      <td>0.844880</td>\n",
       "      <td>0.849982</td>\n",
       "      <td>0.851217</td>\n",
       "      <td>0.850705</td>\n",
       "      <td>0.852515</td>\n",
       "      <td>0.853228</td>\n",
       "      <td>0.852754</td>\n",
       "      <td>0.846233</td>\n",
       "      <td>0.822389</td>\n",
       "      <td>0.823651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe4-wiki_raw_3</th>\n",
       "      <td>0.804249</td>\n",
       "      <td>0.901883</td>\n",
       "      <td>0.904847</td>\n",
       "      <td>0.901687</td>\n",
       "      <td>0.906661</td>\n",
       "      <td>0.887361</td>\n",
       "      <td>0.884942</td>\n",
       "      <td>0.886662</td>\n",
       "      <td>0.906939</td>\n",
       "      <td>0.905052</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881648</td>\n",
       "      <td>0.885157</td>\n",
       "      <td>0.846071</td>\n",
       "      <td>0.850974</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.852074</td>\n",
       "      <td>0.852695</td>\n",
       "      <td>0.853633</td>\n",
       "      <td>0.853050</td>\n",
       "      <td>0.846423</td>\n",
       "      <td>0.822943</td>\n",
       "      <td>0.823581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-raw_4</th>\n",
       "      <td>0.803756</td>\n",
       "      <td>0.882280</td>\n",
       "      <td>0.885303</td>\n",
       "      <td>0.885158</td>\n",
       "      <td>0.886347</td>\n",
       "      <td>0.894502</td>\n",
       "      <td>0.892862</td>\n",
       "      <td>0.895967</td>\n",
       "      <td>0.882639</td>\n",
       "      <td>0.881541</td>\n",
       "      <td>0.881648</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>0.845483</td>\n",
       "      <td>0.851243</td>\n",
       "      <td>0.852220</td>\n",
       "      <td>0.851396</td>\n",
       "      <td>0.849220</td>\n",
       "      <td>0.849588</td>\n",
       "      <td>0.848541</td>\n",
       "      <td>0.847892</td>\n",
       "      <td>0.820586</td>\n",
       "      <td>0.820790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_BERT_3_epoch_1</th>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.891546</td>\n",
       "      <td>0.889776</td>\n",
       "      <td>0.886554</td>\n",
       "      <td>0.890928</td>\n",
       "      <td>0.900654</td>\n",
       "      <td>0.894597</td>\n",
       "      <td>0.899611</td>\n",
       "      <td>0.886186</td>\n",
       "      <td>0.883277</td>\n",
       "      <td>0.885157</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850391</td>\n",
       "      <td>0.852951</td>\n",
       "      <td>0.853902</td>\n",
       "      <td>0.852883</td>\n",
       "      <td>0.850538</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.849595</td>\n",
       "      <td>0.847364</td>\n",
       "      <td>0.828084</td>\n",
       "      <td>0.828082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_BERT_3_cased_decay_epoch_1</th>\n",
       "      <td>0.804277</td>\n",
       "      <td>0.849622</td>\n",
       "      <td>0.848055</td>\n",
       "      <td>0.848332</td>\n",
       "      <td>0.849205</td>\n",
       "      <td>0.849502</td>\n",
       "      <td>0.847476</td>\n",
       "      <td>0.848929</td>\n",
       "      <td>0.845827</td>\n",
       "      <td>0.844880</td>\n",
       "      <td>0.846071</td>\n",
       "      <td>0.845483</td>\n",
       "      <td>0.850391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887492</td>\n",
       "      <td>0.888973</td>\n",
       "      <td>0.887802</td>\n",
       "      <td>0.884318</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.883612</td>\n",
       "      <td>0.879419</td>\n",
       "      <td>0.825163</td>\n",
       "      <td>0.825208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_1</th>\n",
       "      <td>0.809930</td>\n",
       "      <td>0.852054</td>\n",
       "      <td>0.853712</td>\n",
       "      <td>0.853072</td>\n",
       "      <td>0.855006</td>\n",
       "      <td>0.855018</td>\n",
       "      <td>0.853384</td>\n",
       "      <td>0.854534</td>\n",
       "      <td>0.852044</td>\n",
       "      <td>0.849982</td>\n",
       "      <td>0.850974</td>\n",
       "      <td>0.851243</td>\n",
       "      <td>0.852951</td>\n",
       "      <td>0.887492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897313</td>\n",
       "      <td>0.896426</td>\n",
       "      <td>0.891944</td>\n",
       "      <td>0.890019</td>\n",
       "      <td>0.891027</td>\n",
       "      <td>0.888398</td>\n",
       "      <td>0.827446</td>\n",
       "      <td>0.827635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_2</th>\n",
       "      <td>0.809733</td>\n",
       "      <td>0.852460</td>\n",
       "      <td>0.854265</td>\n",
       "      <td>0.854345</td>\n",
       "      <td>0.856488</td>\n",
       "      <td>0.855904</td>\n",
       "      <td>0.854269</td>\n",
       "      <td>0.855426</td>\n",
       "      <td>0.852510</td>\n",
       "      <td>0.851217</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.852220</td>\n",
       "      <td>0.853902</td>\n",
       "      <td>0.888973</td>\n",
       "      <td>0.897313</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.897601</td>\n",
       "      <td>0.891965</td>\n",
       "      <td>0.891721</td>\n",
       "      <td>0.890977</td>\n",
       "      <td>0.887322</td>\n",
       "      <td>0.827386</td>\n",
       "      <td>0.827459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_3</th>\n",
       "      <td>0.808587</td>\n",
       "      <td>0.851922</td>\n",
       "      <td>0.853838</td>\n",
       "      <td>0.853852</td>\n",
       "      <td>0.855840</td>\n",
       "      <td>0.856051</td>\n",
       "      <td>0.853989</td>\n",
       "      <td>0.855193</td>\n",
       "      <td>0.851869</td>\n",
       "      <td>0.850705</td>\n",
       "      <td>0.852074</td>\n",
       "      <td>0.851396</td>\n",
       "      <td>0.852883</td>\n",
       "      <td>0.887802</td>\n",
       "      <td>0.896426</td>\n",
       "      <td>0.897601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892615</td>\n",
       "      <td>0.891410</td>\n",
       "      <td>0.890565</td>\n",
       "      <td>0.887898</td>\n",
       "      <td>0.827181</td>\n",
       "      <td>0.827540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe5-wiki_cased_1</th>\n",
       "      <td>0.805084</td>\n",
       "      <td>0.850317</td>\n",
       "      <td>0.852047</td>\n",
       "      <td>0.851605</td>\n",
       "      <td>0.853060</td>\n",
       "      <td>0.853149</td>\n",
       "      <td>0.851121</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.853302</td>\n",
       "      <td>0.852515</td>\n",
       "      <td>0.852695</td>\n",
       "      <td>0.849220</td>\n",
       "      <td>0.850538</td>\n",
       "      <td>0.884318</td>\n",
       "      <td>0.891944</td>\n",
       "      <td>0.891965</td>\n",
       "      <td>0.892615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892646</td>\n",
       "      <td>0.891905</td>\n",
       "      <td>0.884665</td>\n",
       "      <td>0.824941</td>\n",
       "      <td>0.824532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe5-wiki_cased_2</th>\n",
       "      <td>0.806114</td>\n",
       "      <td>0.849869</td>\n",
       "      <td>0.851872</td>\n",
       "      <td>0.851446</td>\n",
       "      <td>0.853480</td>\n",
       "      <td>0.852943</td>\n",
       "      <td>0.851525</td>\n",
       "      <td>0.851789</td>\n",
       "      <td>0.853201</td>\n",
       "      <td>0.853228</td>\n",
       "      <td>0.853633</td>\n",
       "      <td>0.849588</td>\n",
       "      <td>0.850422</td>\n",
       "      <td>0.882874</td>\n",
       "      <td>0.890019</td>\n",
       "      <td>0.891721</td>\n",
       "      <td>0.891410</td>\n",
       "      <td>0.892646</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892934</td>\n",
       "      <td>0.883555</td>\n",
       "      <td>0.824119</td>\n",
       "      <td>0.824108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe5-wiki_cased_3</th>\n",
       "      <td>0.805870</td>\n",
       "      <td>0.849669</td>\n",
       "      <td>0.851820</td>\n",
       "      <td>0.850658</td>\n",
       "      <td>0.852893</td>\n",
       "      <td>0.852299</td>\n",
       "      <td>0.850733</td>\n",
       "      <td>0.851812</td>\n",
       "      <td>0.853190</td>\n",
       "      <td>0.852754</td>\n",
       "      <td>0.853050</td>\n",
       "      <td>0.848541</td>\n",
       "      <td>0.849595</td>\n",
       "      <td>0.883612</td>\n",
       "      <td>0.891027</td>\n",
       "      <td>0.890977</td>\n",
       "      <td>0.890565</td>\n",
       "      <td>0.891905</td>\n",
       "      <td>0.892934</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883026</td>\n",
       "      <td>0.824389</td>\n",
       "      <td>0.824823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>final-pipe2-cased_10</th>\n",
       "      <td>0.807446</td>\n",
       "      <td>0.847389</td>\n",
       "      <td>0.849091</td>\n",
       "      <td>0.849410</td>\n",
       "      <td>0.851066</td>\n",
       "      <td>0.850682</td>\n",
       "      <td>0.850135</td>\n",
       "      <td>0.850066</td>\n",
       "      <td>0.847535</td>\n",
       "      <td>0.846233</td>\n",
       "      <td>0.846423</td>\n",
       "      <td>0.847892</td>\n",
       "      <td>0.847364</td>\n",
       "      <td>0.879419</td>\n",
       "      <td>0.888398</td>\n",
       "      <td>0.887322</td>\n",
       "      <td>0.887898</td>\n",
       "      <td>0.884665</td>\n",
       "      <td>0.883555</td>\n",
       "      <td>0.883026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.823407</td>\n",
       "      <td>0.823535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_GPT2_CNN_epoch_1</th>\n",
       "      <td>0.811452</td>\n",
       "      <td>0.826767</td>\n",
       "      <td>0.826036</td>\n",
       "      <td>0.825881</td>\n",
       "      <td>0.826834</td>\n",
       "      <td>0.825413</td>\n",
       "      <td>0.823445</td>\n",
       "      <td>0.824968</td>\n",
       "      <td>0.823747</td>\n",
       "      <td>0.822389</td>\n",
       "      <td>0.822943</td>\n",
       "      <td>0.820586</td>\n",
       "      <td>0.828084</td>\n",
       "      <td>0.825163</td>\n",
       "      <td>0.827446</td>\n",
       "      <td>0.827386</td>\n",
       "      <td>0.827181</td>\n",
       "      <td>0.824941</td>\n",
       "      <td>0.824119</td>\n",
       "      <td>0.824389</td>\n",
       "      <td>0.823407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.904372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BERT_exp_GPT2_CNN_seed_epoch_1</th>\n",
       "      <td>0.812112</td>\n",
       "      <td>0.827394</td>\n",
       "      <td>0.826654</td>\n",
       "      <td>0.826499</td>\n",
       "      <td>0.827519</td>\n",
       "      <td>0.826050</td>\n",
       "      <td>0.824052</td>\n",
       "      <td>0.825644</td>\n",
       "      <td>0.824667</td>\n",
       "      <td>0.823651</td>\n",
       "      <td>0.823581</td>\n",
       "      <td>0.820790</td>\n",
       "      <td>0.828082</td>\n",
       "      <td>0.825208</td>\n",
       "      <td>0.827635</td>\n",
       "      <td>0.827459</td>\n",
       "      <td>0.827540</td>\n",
       "      <td>0.824532</td>\n",
       "      <td>0.824108</td>\n",
       "      <td>0.824823</td>\n",
       "      <td>0.823535</td>\n",
       "      <td>0.904372</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         lstm               ...                BERT_exp_GPT2_CNN_seed_epoch_1\n",
       "lstm                                 1.000000               ...                                      0.812112\n",
       "BERT_exp_BERT_3_decay_5_epoch_1      0.806990               ...                                      0.827394\n",
       "final-pipe1-raw_1                    0.807711               ...                                      0.826654\n",
       "final-pipe1-raw_2                    0.808439               ...                                      0.826499\n",
       "final-pipe1-raw_3                    0.809190               ...                                      0.827519\n",
       "final-pipe2-raw_1                    0.806045               ...                                      0.826050\n",
       "final-pipe2-raw_2                    0.806474               ...                                      0.824052\n",
       "final-pipe2-raw_3                    0.806853               ...                                      0.825644\n",
       "final-pipe4-wiki_raw_1               0.806380               ...                                      0.824667\n",
       "final-pipe4-wiki_raw_2               0.805461               ...                                      0.823651\n",
       "final-pipe4-wiki_raw_3               0.804249               ...                                      0.823581\n",
       "final-pipe2-raw_4                    0.803756               ...                                      0.820790\n",
       "BERT_exp_BERT_3_epoch_1              0.809317               ...                                      0.828082\n",
       "BERT_exp_BERT_3_cased_decay_epoch_1  0.804277               ...                                      0.825208\n",
       "final-pipe2-cased_1                  0.809930               ...                                      0.827635\n",
       "final-pipe2-cased_2                  0.809733               ...                                      0.827459\n",
       "final-pipe2-cased_3                  0.808587               ...                                      0.827540\n",
       "final-pipe5-wiki_cased_1             0.805084               ...                                      0.824532\n",
       "final-pipe5-wiki_cased_2             0.806114               ...                                      0.824108\n",
       "final-pipe5-wiki_cased_3             0.805870               ...                                      0.824823\n",
       "final-pipe2-cased_10                 0.807446               ...                                      0.823535\n",
       "BERT_exp_GPT2_CNN_epoch_1            0.811452               ...                                      0.904372\n",
       "BERT_exp_GPT2_CNN_seed_epoch_1       0.812112               ...                                      1.000000\n",
       "\n",
       "[23 rows x 23 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = (\n",
    "    ### LSTM\n",
    "    4.5 * df[\"lstm\"] +\n",
    "    \n",
    "    ### BERT\n",
    "    # best 90%-model\n",
    "    df[\"BERT_exp_BERT_3_decay_5_epoch_1\"] +\n",
    "    df[\"BERT_exp_BERT_3_cased_decay_epoch_1\"] +\n",
    "    \n",
    "    # GPT-CNN\n",
    "    2 * df[\"BERT_exp_GPT2_CNN_epoch_1\"] + \n",
    "    2 * df[\"BERT_exp_GPT2_CNN_seed_epoch_1\"] +\n",
    "#     2 * df[\"final-pipe6-gpt_wiki_1\"] +\n",
    "    \n",
    "    # fint-tune\n",
    "    df[\"final-pipe1-raw_1\"] +\n",
    "    df[\"final-pipe1-raw_2\"] +\n",
    "    df[\"final-pipe1-raw_3\"] +\n",
    "    # base-uncased\n",
    "    df[\"final-pipe2-raw_1\"] +\n",
    "    df[\"final-pipe2-raw_2\"] +\n",
    "    df[\"final-pipe2-raw_3\"] +\n",
    "    # cased (note: 4,5,6 don't really improve results)\n",
    "    df[\"final-pipe2-cased_1\"] +\n",
    "    df[\"final-pipe2-cased_2\"] +\n",
    "    df[\"final-pipe2-cased_3\"] + \n",
    "#     df[\"final-pipe2-cased_4\"] +\n",
    "#     df[\"final-pipe2-cased_5\"] +\n",
    "#     df[\"final-pipe2-cased_6\"] +\n",
    "    # Wiki-ft\n",
    "    df[\"final-pipe4-wiki_raw_1\"] +\n",
    "    df[\"final-pipe4-wiki_raw_2\"] +\n",
    "    df[\"final-pipe4-wiki_raw_3\"] +\n",
    "    # Wiki-cased\n",
    "    df[\"final-pipe5-wiki_cased_1\"] +\n",
    "    df[\"final-pipe5-wiki_cased_2\"] +\n",
    "    df[\"final-pipe5-wiki_cased_3\"] + \n",
    "    \n",
    "    df[\"final-pipe2-raw_4\"] +\n",
    "    df[\"final-pipe2-cased_10\"] +\n",
    "    df[\"BERT_exp_BERT_3_epoch_1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'prediction': final_preds,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7097320</td>\n",
       "      <td>0.266914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7097321</td>\n",
       "      <td>2.494023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7097322</td>\n",
       "      <td>8.426736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7097323</td>\n",
       "      <td>1.454720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7097324</td>\n",
       "      <td>0.546256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7097320    0.266914\n",
       "1  7097321    2.494023\n",
       "2  7097322    8.426736\n",
       "3  7097323    1.454720\n",
       "4  7097324    0.546256"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2683.1455078125\n",
      "6458.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.max_memory_allocated(0) / 1024 / 1024)\n",
    "print(torch.cuda.max_memory_cached(0) / 1024 / 1024)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1375107,
     "sourceId": 12500,
     "sourceType": "competition"
    },
    {
     "datasetId": 207164,
     "sourceId": 505147,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 243479,
     "sourceId": 515048,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 236257,
     "sourceId": 516430,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 241628,
     "sourceId": 516500,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 226782,
     "sourceId": 563045,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 27938,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2ef73e2d8c5049ce810360bb86a40940": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5dcc22d0866741f496171bfb934934f3",
        "IPY_MODEL_958033834a5641dfbc14e7774bcaef55"
       ],
       "layout": "IPY_MODEL_dc66dc54b7bc450e957dcea73aa6e9a5"
      }
     },
     "413cedc04d434b7996dd8fa53d09acdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "51d4baedd8db47a1821b7204d341d644": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5dcc22d0866741f496171bfb934934f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_75bcd3df05964fafb4e0c644cd591d32",
       "max": 1521,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f34fbad09d024e4dacbff6e49cb08465",
       "value": 1521
      }
     },
     "75bcd3df05964fafb4e0c644cd591d32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "958033834a5641dfbc14e7774bcaef55": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_51d4baedd8db47a1821b7204d341d644",
       "placeholder": "​",
       "style": "IPY_MODEL_e5014a071523488d810bee510426b075",
       "value": "100% 1521/1521 [13:42&lt;00:00,  2.19it/s]"
      }
     },
     "af27b39960e14e3ba4a10fbb7ebb5537": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b793ca5992ee4e8bbe036c874914f68a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c1577e0de2ce4f23a7d0c48306364eaf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d67a99a6b61a44ec84f666a0518f0be3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc66dc54b7bc450e957dcea73aa6e9a5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3387ba9000144b89e2854d3a065a590": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f48339f077ad4181af835f3e1fc4108e",
        "IPY_MODEL_f5a3a1dd2637419bb748f01410eab086"
       ],
       "layout": "IPY_MODEL_af27b39960e14e3ba4a10fbb7ebb5537"
      }
     },
     "e5014a071523488d810bee510426b075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f34fbad09d024e4dacbff6e49cb08465": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f48339f077ad4181af835f3e1fc4108e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d67a99a6b61a44ec84f666a0518f0be3",
       "max": 1521,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b793ca5992ee4e8bbe036c874914f68a",
       "value": 1521
      }
     },
     "f5a3a1dd2637419bb748f01410eab086": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c1577e0de2ce4f23a7d0c48306364eaf",
       "placeholder": "​",
       "style": "IPY_MODEL_413cedc04d434b7996dd8fa53d09acdd",
       "value": "100% 1521/1521 [13:42&lt;00:00,  2.19it/s]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
